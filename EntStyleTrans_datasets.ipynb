{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Yelp data\n",
    "\n",
    "path = 'language-style-transfer/data/yelp/'\n",
    "yelp_list = [path+'sentiment.train.0',path+'sentiment.train.1',path+'sentiment.dev.0',path+'sentiment.dev.1',\n",
    "            path+'sentiment.test.0',path+'sentiment.test.1']\n",
    "\n",
    "# Amazon data (already a dict)\n",
    "\n",
    "amazon_file = 'text_style_transfer/model/data/q_train.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yelp contains 5664380 total tokens\n",
      "Amazon contains 8245755 total tokens\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"process the raw text into one list containing all tokens\"\"\"\n",
    "    data = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                data.append(token.lower())\n",
    "    return data\n",
    "\n",
    "# raw file for all Yelp data\n",
    "\n",
    "yelp_rawtext = []\n",
    "\n",
    "for file in yelp_list:\n",
    "    yelp_rawtext += read_data(file)\n",
    "    \n",
    "# raw file for all Amazon data\n",
    "    \n",
    "amazon_rawtext = read_data(amazon_file)\n",
    "\n",
    "print(\"Yelp contains {} total tokens\".format(len(yelp_rawtext)))\n",
    "print(\"Amazon contains {} total tokens\".format(len(amazon_rawtext)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 0], ('.', 496255), ('the', 235199), ('and', 179919), ('i', 136692)]\n",
      "Sample data [4, 7, 963, 7460, 1, 33, 42, 10, 2, 3741] ['i', 'was', 'sadly', 'mistaken', '.', 'so', 'on', 'to', 'the', 'hoagies']\n",
      "Created Yelp dictionary...\n",
      "Most common words (+UNK) [['UNK', 10267], ('.', 585450), ('the', 352597), ('i', 285028), ('it', 230472)]\n",
      "Sample data [442, 21, 3000, 289, 2, 4572, 28, 2144, 1, 3] ['especially', 'on', 'moderate', 'where', 'the', 'attacks', 'are', 'constant', '.', 'i']\n",
      "Created Amazon dictionary...\n",
      "9650\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = {}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "for index,words in enumerate([yelp_rawtext,amazon_rawtext]):\n",
    "    #print(\"Generating dictionary for {} raw text...\".format(words))\n",
    "    data, count, unused_dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n",
    "    del words  # Hint to reduce memory.\n",
    "    print('Most common words (+UNK)', count[:5])\n",
    "    print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "    if index == 0: #yelp\n",
    "        print(\"Created Yelp dictionary...\")\n",
    "        yelp_dict = unused_dictionary\n",
    "    elif index == 1: #amazon\n",
    "        print(\"Created Amazon dictionary...\")\n",
    "        amazon_dict = unused_dictionary\n",
    "\n",
    "print(len(yelp_dict))\n",
    "print(len(amazon_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
