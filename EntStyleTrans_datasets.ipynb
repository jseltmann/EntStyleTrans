{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "# Yelp data\n",
    "\n",
    "#path = 'language-style-transfer/data/yelp/'\n",
    "path = 'data/yelp/'\n",
    "yelp_list = [path+'sentiment.train.0',path+'sentiment.train.1',path+'sentiment.dev.0',path+'sentiment.dev.1',\n",
    "            path+'sentiment.test.0',path+'sentiment.test.1']\n",
    "\n",
    "# Amazon data (already a dict)\n",
    "\n",
    "#amazon_file = 'text_style_transfer/model/data/q_train.txt'\n",
    "amazon_file = 'data/amazon/q_train.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"process the raw text into one list containing all tokens\"\"\"\n",
    "    data = []\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            tokens = [x.lower() for x in line.split()]\n",
    "            tokens = [\"<START>\"] + tokens + [\"<END>\"]\n",
    "            pad_len = max_len - len(tokens)\n",
    "            if pad_len < 0:\n",
    "                #very brutal, but shouldn't affect us, because none of the sentences in yelp are longer than 17\n",
    "                tokens = tokens[:max_len]\n",
    "            else:\n",
    "                tokens = tokens + [\"<PAD>\"] * pad_len\n",
    "            data.append(tokens)\n",
    "            #for token in tokens:\n",
    "            #    data.append(token.lower())\n",
    "    return data\n",
    "\n",
    "# raw file for all Yelp data\n",
    "\n",
    "yelp_rawtext_train = []\n",
    "yelp_rawtext_dev = []\n",
    "yelp_rawtext_test = []\n",
    "\n",
    "for file in yelp_list[:2]:\n",
    "    yelp_rawtext_train += read_data(file)\n",
    "for file in yelp_list[2:4]:\n",
    "    yelp_rawtext_dev += read_data(file)\n",
    "for file in yelp_list[4:6]:\n",
    "    yelp_rawtext_test += read_data(file)\n",
    "    \n",
    "# raw file for all Amazon data\n",
    "    \n",
    "#amazon_rawtext = read_data(amazon_file)\n",
    "\n",
    "#print(\"Yelp contains {} total tokens\".format(len(yelp_rawtext)))\n",
    "#print(\"Amazon contains {} total tokens\".format(len(amazon_rawtext)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "\n",
    "def build_dataset(sents, word2num, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    #count = [['UNK', -1]]\n",
    "    #words = [word for sent in sents for word in sent]\n",
    "    #count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    #dictionary = {}\n",
    "    #for word, _ in count:\n",
    "    #    dictionary[word] = len(dictionary)\n",
    "    data = []\n",
    "    #unk_count = 0\n",
    "    #for word in words:\n",
    "    for sent in sents:\n",
    "        inds = [word2num[word] for word in sent]\n",
    "        #index = dictionary.get(word, 0)\n",
    "        #if index == 0:  # dictionary['UNK']\n",
    "        #    unk_count += 1\n",
    "        #unk_count += sum([i == 0 for i in inds])\n",
    "        data.append(inds)\n",
    "    data = [np.array(s) for s in data]\n",
    "    #count[0][1] = unk_count\n",
    "    #reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data#, count, dictionary, reversed_dictionary\n",
    "\n",
    "#for index,words in enumerate([yelp_rawtext]):#,amazon_rawtext]):\n",
    "    #print(\"Generating dictionary for {} raw text...\".format(words))\n",
    "#    data, count, unused_dictionary, reverse_dictionary = build_dataset(words, vocabulary_size)\n",
    "#    del words  # Hint to reduce memory.\n",
    "#    print('Most common words (+UNK)', count[:5])\n",
    "#    print('Sample data', data[0][:10], [reverse_dictionary[i] for i in data[0][:10]])\n",
    "#    if index == 0: #yelp\n",
    "#        print(\"Created Yelp dictionary...\")\n",
    "#        yelp_dict = unused_dictionary\n",
    "#    elif index == 1: #amazon\n",
    "#        print(\"Created Amazon dictionary...\")\n",
    "#        amazon_dict = unused_dictionary\n",
    "\n",
    "#print(len(yelp_dict))\n",
    "#print(len(amazon_dict))\n",
    "words = set([word for split in [yelp_rawtext_train, yelp_rawtext_dev, yelp_rawtext_test]\n",
    "        for sent in split for word in sent])\n",
    "words.add(\"UNK\")\n",
    "\n",
    "\n",
    "word2num = dict([(w,i) for (i,w) in enumerate(words)])\n",
    "num2word = dict([(i,w) for (w,i) in word2num.items()])\n",
    "\n",
    "train_data = build_dataset(yelp_rawtext_train, word2num, vocabulary_size)\n",
    "test_data = build_dataset(yelp_rawtext_test, word2num, vocabulary_size)\n",
    "dev_data = build_dataset(yelp_rawtext_dev, word2num, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_data_path = \"data/yelp_train.pkl\"\n",
    "dev_data_path = \"data/yelp_dev.pkl\"\n",
    "test_data_path = \"data/yelp_test.pkl\"\n",
    "word_ind_path = \"data/yelp_word_inds.pkl\"\n",
    "\n",
    "with open(train_data_path, \"wb\") as tdf:\n",
    "  pickle.dump(train_data, tdf)\n",
    "with open(dev_data_path, \"wb\") as ddf:\n",
    "  pickle.dump(dev_data, ddf)\n",
    "with open(test_data_path, \"wb\") as tdf:\n",
    "  pickle.dump(test_data, tdf)\n",
    "\n",
    "word_inds = dict()\n",
    "word_inds[\"word2num\"] = word2num\n",
    "word_inds[\"num2word\"] = num2word\n",
    "with open(word_ind_path, \"wb\") as wip:\n",
    "  pickle.dump(word_inds, wip)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
